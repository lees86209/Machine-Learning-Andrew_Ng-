import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
import scipy.optimize as optimize
from sklearn.metrics import classification_report # evaluate report


os.chdir("D:/BOOKS/Machine Learning/Andrew Ng/Week5/machine-learning-ex4/ex4")

""" 1. Neural Networks """
""" 1.1 Visualizing the data """
# there are 5,000 training sample, and each sample is 20*20 pixel image
# so that are 5000 * 400 metrix

def load_mat(path):
    data = loadmat("ex4data1.mat")
    X = data["X"]
    y = data["y"].flatten()
    
    return X, y

def plot_100_images(X):
    """ randomly plot 100 digit """
    index = np.random.choice(range(5000), 100)
    images = X[index]
    fig, ax_array = plt.subplots(10, 10, sharey = True, sharex = True, figsize = (8,8))
    for r in range(10):
        for c in range(10):
            ax_array[r, c].matshow(images[r*10 +c].reshape(20, 20), cmap="gray_r")
    plt.xticks([])
    plt.yticks([])
    plt.show()

X, y = load_mat("ex4data1.mat")
plot_100_images(X)


""" 1.2 Model representation """
""" 1.2.1 load train data set """
#ex. y[0]=6 transfor y[0]=[0,0,0,0,0,1,0,0,0,0]
from sklearn.preprocessing import OneHotEncoder
def expand_y(y):    
    result = []
    for i in y:
        y_array = np.zeros(10)
        y_array[i-1] = 1
        result.append(y_array)
    """
    encoder =  OneHotEncoder(sparse=False)  # return a array instead of matrix
    y_onehot = encoder.fit_transform(y.reshape(-1,1))
    return y_onehot
    """
    return np.array(result)

raw_X, raw_y = load_mat("ex4data1.mat")
X = np.insert(raw_X, 0, 1, axis = 1)
y = expand_y(raw_y)
X.shape, y.shape


""" 1.2.2 load weight """
def load_weight(path):
    data = loadmat(path)
    return data["Theta1"], data["Theta2"]

t1, t2 = load_weight("ex4weights.mat")
t1.shape, t2.shape


""" 1.2.3 expand parameter """
def serialize(a, b):
    return np.r_[a.flatten(), b.flatten()]

theta = serialize(t1, t2)
theta.shape

def deserialize(seq):
    # grab the parameter
    return seq[:25*401].reshape(25,401), seq[25*401:].reshape(10, 26)


""" 1.3 Feedforward and cost function """
""" 1.3.1 Feedforward """
# make sure each layer add 1 unit (value = 1) for the theta_0

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def feed_forward(theta, X):
    """ return each input and output """
    t1, t2 = deserialize(theta)
    
    a1 = X
    z2 = a1 @ t1.T
    a2 = np.insert(sigmoid(z2), 0, 1, axis =1)
    z3 = a2 @ t2.T
    a3 = sigmoid(z3)
    
    return a1, z2, a2, z3, a3

a1, z2, a2, z3, h = feed_forward(theta, X)

""" 1.3.2 cost function """
def cost(theta, X, y):
    a1, z2, a2, z3, h = feed_forward(theta, X)
    J = 0
    for i in range(len(X)):
        first = - y[i] * np.log(h[i])
        second = (1 - y[i]) * np.log(1 - h[i])
        J = J + np.sum(first - second)
    J = J / len(X)
    return J
"""
J = - y * np.log(h) - (1 - y) * np.log(1 - h)
     return J.sum() / len(X)
"""

cost(theta, X, y)


""" 1.4 Regularized cost function """
def regularized_cost(theta, X, y, l=1):
    '''retularizing will ignore first column of parameter matrix'''
    t1, t2 = deserialize(theta)
    reg = np.sum(t1[:,1:] ** 2) + np.sum(t2[:,1:] ** 2)  # or use np.power(a, 2)
    return l / (2 * len(X)) * reg + cost(theta, X, y)

regularized_cost(theta, X, y, 1)


""" 2 Backpropagation """
""" 2.1 Sigmoid gradient """
def sigmoid_gradient(z):
    return sigmoid(z) * (1 - sigmoid(z))

""" 2.2 Random initialization """
# choose the random value from (-e, e)
# choose e = 0.12 vause this range can make sure the parameter is small enough that can make it more efficient

def random_init(size):
    return np.random.uniform(-0.12, 0.12, size)

""" 2.3 Backpropagation """
print("a1", a1.shape, "t1", t1.shape)
print("z2", z2.shape)
print("a2", a2.shape, "t2", t2.shape)
print("z3", z3.shape)
print("a3", h.shape)

def gradient(theta, X, y):
    """ return all the gradient of theta, D(i) must equal theta(i).shape """
    t1, t2 = deserialize(theta)
    a1, z2, a2, z3, h = feed_forward(theta, X)
    d3 = h - y #(5000, 10)
    d2 = d3 @ t2[:, 1:] * sigmoid_gradient(z2) # (5000, 25)
    D2 = d3.T @ a2 # (10, 26)
    D1 = d2.T @ a1 # (25,401)
    D = (1 / len(X)) * serialize(D1, D2) # (10285)
    
    return D


t1, t2 = deserialize(theta)
a1, z2, a2, z3, h = feed_forward(theta, X)
d3 = h - y #(5000, 10)
d2 = d3 @ t2[:, 1:] * sigmoid_gradient(z2) # (5000, 25)
D2 = d3.T @ a2 # (10, 26)
D1 = d2.T @ a1 # (25,401)
D = (1 / len(X)) * serialize(D1, D2) # (10285)

""" 2.4 Gradient checking """
def gradient_checking(theta, X, y, e):
    def a_numeric_grad(plus, minus):
        # calculate each graident value of theta
        return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (e * 2)
   
    numeric_grad = [] 
    for i in range(len(theta)):
        plus = theta.copy()  # deep copy otherwise you will change the raw theta
        minus = theta.copy()
        plus[i] = plus[i] + e
        minus[i] = minus[i] - e
        grad_i = a_numeric_grad(plus, minus)
        numeric_grad.append(grad_i)
    
    numeric_grad = np.array(numeric_grad)
    analytic_grad = regularized_gradient(theta, X, y)
    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)

    print('If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: {}\n'.format(diff))
    
gradient_checking(theta, X, y, e = 0.0001)
# this will take few minutes
def regularized_gradient(theta, X, y, l=1):

    a1, z2, a2, z3, h = feed_forward(theta, X)
    D1, D2 = deserialize(gradient(theta, X, y))
    t1[:,0] = 0
    t2[:,0] = 0
    reg_D1 = D1 + (l / len(X)) * t1
    reg_D2 = D2 + (l / len(X)) * t2
    
    return serialize(reg_D1, reg_D2)

""" 2.6 learning parameters using fmincg """
def nn_training(X, y):
    init_theta = random_init(10285)  # 25*401 + 10*26

    res = opt.minimize(fun=regularized_cost,
                       x0=init_theta,
                       args=(X, y, 1),
                       method='TNC',
                       jac=regularized_gradient,
                       options={'maxiter': 400})
    return res

res = nn_training(X, y)
# this will take few minutes
res


def accuracy(theta, X, y):
    _, _, _, _, h = feed_forward(res.x, X)
    y_pred = np.argmax(h, axis=1) + 1
    print(classification_report(y, y_pred))
    
accuracy(res.x, X, raw_y)


""" 3. Visualizing the hidden layer """
def plot_hidden(theta):
    t1, _ = deserialize(theta)
    t1 = t1[:, 1:]
    fig,ax_array = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(6,6))
    for r in range(5):
        for c in range(5):
            ax_array[r, c].matshow(t1[r * 5 + c].reshape(20, 20), cmap='gray_r')
            plt.xticks([])
            plt.yticks([])
    plt.show()

plot_hidden(res.x)
